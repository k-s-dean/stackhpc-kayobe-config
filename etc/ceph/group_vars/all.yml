# ceph-ansible group vars

ansible_user: centos

ceph_origin: repository
ceph_repository: community

ceph_stable_release: "nautilus"

public_network: "10.10.44.0/22"
cluster_network: "10.10.48.0/22"
rgw_loadbalancer_network: "45.135.56.0/24"

# If configure_firewall is true, then ansible will try to configure the
# appropriate firewalling rules so that Ceph daemons can communicate
# with each others.
configure_firewall: true

# Open ports on corresponding nodes if firewall is installed on it
ceph_mon_firewall_zone: storage
ceph_mgr_firewall_zone: storage
ceph_osd_firewall_zone: storage
ceph_rgw_firewall_zone: storage
#ceph_mds_firewall_zone: public
#ceph_nfs_firewall_zone: public
#ceph_rbdmirror_firewall_zone: public
#ceph_iscsi_firewall_zone: public
#ceph_dashboard_firewall_zone: public
ceph_rgwloadbalancer_firewall_zone: public
ceph_rgwloadbalancer_vrrp_firewall_zone: storage

docker: true
containerized_deployment: true
container_binary: "docker"
ceph_docker_version: "19.03.8"
docker_pull_timeout: "600s"
# Tags can be found on Dockerhub:
# https://hub.docker.com/r/ceph/daemon/tags?page=1&ordering=last_updated&name=nautilus
ceph_docker_image_tag: "v4.0.21-stable-4.0-nautilus-centos-7"

dashboard_enabled: false

osd_pool_default_pg_num: 512

openstack_config: true

openstack_pools:
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_cinder_pool }}"
  - "{{ openstack_cinder_backup_pool }}"
  - "{{ openstack_gnocchi_pool }}"
  - "{{ openstack_nova_pool }}"

openstack_glance_pool:
  name: "images"
  pg_num: "{{ osd_pool_default_pg_num }}"
  pgp_num: "{{ osd_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  pg_autoscale_mode: False
openstack_cinder_pool:
  name: "volumes"
  pg_num: "{{ osd_pool_default_pg_num }}"
  pgp_num: "{{ osd_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  pg_autoscale_mode: False
openstack_nova_pool:
  name: "vms"
  pg_num: "{{ osd_pool_default_pg_num }}"
  pgp_num: "{{ osd_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  pg_autoscale_mode: False
openstack_cinder_backup_pool:
  name: "backups"
  pg_num: "{{ osd_pool_default_pg_num }}"
  pgp_num: "{{ osd_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  pg_autoscale_mode: False
openstack_gnocchi_pool:
  name: "metrics"
  pg_num: "{{ osd_pool_default_pg_num }}"
  pgp_num: "{{ osd_pool_default_pg_num }}"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  pg_autoscale_mode: False

openstack_keys:
  - { name: client.glance, caps: { mon: "profile rbd", osd: "profile rbd pool=volumes, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }
  - { name: client.cinder, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }
  - { name: client.cinder-backup, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_backup_pool.name }}"}, mode: "0600" }
  - { name: client.gnocchi, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_gnocchi_pool.name }}"}, mode: "0600", }
  - { name: client.nova, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }

osd_auto_discovery: true

ceph_conf_overrides:
  global:
    osd_pool_default_size: 2
    osd_pool_default_min_size: 2
  "client.rgw.{{ inventory_hostname }}.rgw0":
    "rgw keystone api version": "3"
    "rgw keystone url": "{{ rgw_keystone_url }}"
    "rgw keystone admin user": "ceph_rgw"
    "rgw keystone admin password": "{{ rgw_keystone_password }}"
    "rgw keystone admin project": "service"
    "rgw keystone admin domain": "default"
    "rgw keystone accepted roles": "member, Member, _member_, admin"
    "rgw keystone accepted admin roles": "admin"
    "rgw keystone token cache size": "10000"
    "rgw keystone revocation interval": "900"
    "rgw keystone verify ssl": "false"
    "rgw s3 auth use keystone": "true"
    "rgw swift account in url": "true"

rgw_keystone_url: "https://10.10.39.254:5000"

rgw_keystone_password: !vault |
  $ANSIBLE_VAULT;1.1;AES256
  31363337633238663739313733643938633130313430616665663932313662356236353831623737
  6461646632666331316531323764336136373434363762630a626163306565343539663066646431
  63333361336636303965356564626362303232366439373830633635663337316432303638613537
  6639623063366131640a383066656536396335346137663336323038303565326636366636346339
  62386534346431373031366632663462643531356665396335633331653134333061623264323165
  3130366663346135313638653863313235346639396537353032
